
# SQL
# Найти пользователя, который чаще остальных реагирует на упоминания его в сообщениях.

import findspark
findspark.init()
findspark.find()

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("csv_processing").getOrCreate()

df = spark.read.csv("ira_tweets_csv_hashed.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("tweets")

result = spark.sql("""
    SELECT userid, COUNT(*) AS count 
    FROM (
        SELECT DISTINCT users.userid, users.in_reply_to_tweetid 
        FROM (
            SELECT userid, in_reply_to_tweetid 
            FROM tweets 
            WHERE in_reply_to_tweetid IS NOT NULL
        ) AS users
        JOIN (
            SELECT tweetid, user_mentions 
            FROM tweets 
            WHERE user_mentions IS NOT NULL
        ) AS mentions
        ON mentions.tweetid = users.in_reply_to_tweetid
        WHERE mentions.user_mentions LIKE CONCAT('%', users.userid, '%')
    ) AS filtered
    GROUP BY userid 
    ORDER BY count DESC
    LIMIT 1
""")

result.select('userid').show()

spark.stop()

---------------------------


# RDD + Граф
# Найти пользователя, который чаще остальных реагирует на упоминания его в сообщениях.

import findspark
findspark.init()
findspark.find()

from pyspark import SparkContext
import networkx as nx
import matplotlib.pyplot as plt

sc = SparkContext(appName="csv_processing")

rdd = sc.textFile("ira_tweets_csv_hashed.csv")

header = rdd.first()
columns = header.split(',')

G = nx.DiGraph()

G.add_node("rdd: raw data")
data_rdd = rdd.filter(lambda line: line != header).map(lambda line: line.split(','))
G.add_node("data_rdd: split columns")
G.add_edge("rdd: raw data", "data_rdd: split columns")

tweetid_idx = 0
userid_idx = 1
in_reply_to_tweedid_idx = 15
user_mentions_idx = 29

users_rdd = data_rdd.filter(lambda x: x[in_reply_to_tweedid_idx] is not None).map(lambda x: (x[in_reply_to_tweedid_idx], x[userid_idx]))
G.add_node("users_rdd: filtered")
G.add_edge("data_rdd: split columns", "users_rdd: filtered")

mentions_rdd = data_rdd.filter(lambda x: x[user_mentions_idx] is not None).map(lambda x: (x[tweetid_idx], x[user_mentions_idx]))
G.add_node("mentions_rdd: filtered")
G.add_edge("data_rdd: split columns", "mentions_rdd: filtered")

joined_rdd = users_rdd.join(mentions_rdd)
G.add_node("joined_rdd: joined")
G.add_edge("users_rdd: filtered", "joined_rdd: joined")
G.add_edge("mentions_rdd: filtered", "joined_rdd: joined")

filtered_joined_rdd = joined_rdd.filter(lambda x: x[1][1].find((x[1][0])[1:len(x[1][0])-1]) != -1)
G.add_node("filtered_joined_rdd: filtered")
G.add_edge("joined_rdd: joined", "filtered_joined_rdd: filtered")

count_rdd = filtered_joined_rdd.map(lambda x: (x[1][0], 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: -x[1])
G.add_node("count_rdd: counted")
G.add_edge("filtered_joined_rdd: filtered", "count_rdd: counted")

for result in count_rdd.take(1):
    print(result[0])

pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, node_size=3000, node_color="skyblue", font_size=10, font_weight="bold", arrows=True)
plt.title('RDD Graph')
plt.show()

sc.stop()

--------------------------


# GraphFrame
# Найти наибольшую компоненту связности социального графа (группу пользователей, которые общаются преимущественно друг с другом) 
# для российских пользователей.

import findspark
findspark.init()
findspark.find()

from graphframes import *
from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("csv_processing")
sc = SparkContext(conf=conf)


spark = SparkSession.builder.appName("csv_processing").getOrCreate()

df = spark.read.csv("ira_tweets_csv_hashed.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("tweets")

cleaned_tweets = spark.sql("SELECT * FROM tweets WHERE user_reported_location LIKE '%Россия%'")
cleaned_tweets.createOrReplaceTempView("clean")

v = spark.sql("SELECT DISTINCT userid AS id FROM clean")
v.show()
e = spark.sql("SELECT DISTINCT userid AS src, in_reply_to_userid AS dst FROM clean WHERE NOT in_reply_to_userid IS NULL")
e.show()

g = GraphFrame(v, e)

sc.setCheckpointDir("/tmp")

components = g.connectedComponents()
components.show()
components.createOrReplaceTempView("comp")

spark.sql("SELECT component, COUNT(*) AS count FROM comp GROUP BY component ORDER BY count DESC LIMIT 1").show()

sc.stop()
spark.stop()

---------------------------


# SQL
# Найти пользователя из РФ, чаще остальных упоминающего фамилии российских политических деятелей (на русском).

import findspark
findspark.init()
findspark.find()

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("csv_processing").getOrCreate()

df = spark.read.csv("ira_tweets_csv_hashed.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("tweets")

politics = ["Путин", "Жириновский", "Зюганов", "Песков", "Лавров", "Пригожин", "Мишустин", "Шойгу", "Медведев", "Матвиенко"]

query = "SELECT userid, COUNT(*) as count FROM tweets WHERE account_language = 'ru' AND tweet_language = 'ru' AND ("
query += f"tweet_text LIKE '%{politics[0]}%'"
for p in politics[1:]:
    query += f" OR tweet_text LIKE '%{p}%'"
query += ') GROUP BY userid ORDER BY count DESC LIMIT 1'

result = spark.sql(query)
result.select('userid').show()

spark.stop()

---------------------------


# RDD
# Найти пользователя из РФ, чаще остальных упоминающего фамилии российских политических деятелей (на русском).

import findspark
findspark.init()
findspark.find()

from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("csv_processing")
sc = SparkContext(conf=conf)

file_path = "ira_tweets_csv_hashed.csv"
rdd = sc.textFile(file_path)

header = rdd.first()
columns = header.split(',')

data_rdd = rdd.filter(lambda line: line != header).map(lambda line: line.split(','))

userid_idx = 1
account_language_idx = 10
tweet_language_idx = 11
tweet_text_idx = 12

politics = ["Путин", "Жириновский", "Зюганов", "Песков", "Лавров", "Пригожин", "Мишустин", "Шойгу", "Медведев", "Матвиенко"]

def contains_politics(text):
    return any(politician in text for politician in politics)

filtered_rdd = data_rdd.filter(lambda x: x[account_language_idx] == '"ru"' and x[tweet_language_idx] == '"ru"' and contains_politics(x[tweet_text_idx])) \
                       .map(lambda x: (x[userid_idx], 1)) \
                       .reduceByKey(lambda a, b: a + b) \
                       .sortBy(lambda x: -x[1])

for result in filtered_rdd.take(1):
    print(result[0])

sc.stop()

---------------------------


# GraphFrame
# Найти пользователя, начавшего n-ю по количеству сообщений непрерывную цепочку.

import findspark
findspark.init()
findspark.find()

from graphframes import *
from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("csv_processing")
sc = SparkContext(conf=conf)


spark = SparkSession.builder.appName("csv_processing").getOrCreate()

df = spark.read.csv("ira_tweets_csv_hashed.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("tweets")


v = spark.sql("""SELECT DISTINCT userid AS id FROM tweets""")
v.show()

e = spark.sql("""SELECT DISTINCT userid AS src, in_reply_to_userid AS dst, tweetid FROM tweets""")
e.createOrReplaceTempView("edges")
e.show()

graph = GraphFrame(v, e)

sc.setCheckpointDir("/tmp")

scc_result = graph.stronglyConnectedComponents(maxIter=2)
scc_result.show()
scc_result.createOrReplaceTempView("scc_result")

n = 3  # Параметр n, указывающий номер цепочки по длине
component_sizes = spark.sql(f"""
    SELECT component, COUNT(*) as count 
    FROM scc_result 
    GROUP BY component
    ORDER BY count DESC
    LIMIT {n}
""")
component_sizes.createOrReplaceTempView("component_sizes")
component_sizes.show()

nth_largest_component = spark.sql(f"""
    SELECT component
    FROM (
        SELECT component,count, ROW_NUMBER() OVER (ORDER BY count DESC) as row_num
        FROM component_sizes
    ) WHERE row_num = {n}
""").collect()[0]["component"]

users_in_nth_largest_component = spark.sql(f"""
    SELECT id
    FROM scc_result
    WHERE component = {nth_largest_component}
""")

users_in_nth_largest_component.createOrReplaceTempView("users")

starting_user = spark.sql("""
    SELECT *
    FROM users
    LEFT JOIN edges ON users.id = edges.src
    WHERE edges.dst IS NULL LIMIT 1
""")

# Вывод результата
starting_user.show()

# Остановка SparkSession
spark.stop()
