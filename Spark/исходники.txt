
# SQL
# Найти пользователя, который чаще остальных реагирует на упоминания его в сообщениях.

import findspark
findspark.init()
findspark.find()

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("csv_processing").getOrCreate()

df = spark.read.csv("ira_tweets_csv_hashed.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("tweets")

result = spark.sql("""
    SELECT userid, COUNT(*) AS count 
    FROM (
        SELECT DISTINCT users.userid, users.in_reply_to_tweetid 
        FROM (
            SELECT userid, in_reply_to_tweetid 
            FROM tweets 
            WHERE in_reply_to_tweetid IS NOT NULL
        ) AS users
        JOIN (
            SELECT tweetid, user_mentions 
            FROM tweets 
            WHERE user_mentions IS NOT NULL
        ) AS mentions
        ON mentions.tweetid = users.in_reply_to_tweetid
        WHERE mentions.user_mentions LIKE CONCAT('%', users.userid, '%')
    ) AS filtered
    GROUP BY userid 
    ORDER BY count DESC
    LIMIT 1
""")

result.select('userid').show()

spark.stop()

---------------------------

# RDD + Граф
# Найти пользователя, который чаще остальных реагирует на упоминания его в сообщениях.

import findspark
findspark.init()
findspark.find()

from pyspark import SparkContext
import networkx as nx
import matplotlib.pyplot as plt

sc = SparkContext(appName="csv_processing")

rdd = sc.textFile("ira_tweets_csv_hashed.csv")

header = rdd.first()
columns = header.split(',')

G = nx.DiGraph()

G.add_node("rdd: raw data")
data_rdd = rdd.filter(lambda line: line != header).map(lambda line: line.split(','))
G.add_node("data_rdd: split columns")
G.add_edge("rdd: raw data", "data_rdd: split columns")

tweetid_idx = 0
userid_idx = 1
in_reply_to_tweedid_idx = 15
user_mentions_idx = 29

users_rdd = data_rdd.filter(lambda x: x[in_reply_to_tweedid_idx] is not None).map(lambda x: (x[in_reply_to_tweedid_idx], x[userid_idx]))
G.add_node("users_rdd: filtered")
G.add_edge("data_rdd: split columns", "users_rdd: filtered")

mentions_rdd = data_rdd.filter(lambda x: x[user_mentions_idx] is not None).map(lambda x: (x[tweetid_idx], x[user_mentions_idx]))
G.add_node("mentions_rdd: filtered")
G.add_edge("data_rdd: split columns", "mentions_rdd: filtered")

joined_rdd = users_rdd.join(mentions_rdd)
G.add_node("joined_rdd: joined")
G.add_edge("users_rdd: filtered", "joined_rdd: joined")
G.add_edge("mentions_rdd: filtered", "joined_rdd: joined")

filtered_joined_rdd = joined_rdd.filter(lambda x: x[1][1].find((x[1][0])[1:len(x[1][0])-1]) != -1)
G.add_node("filtered_joined_rdd: filtered")
G.add_edge("joined_rdd: joined", "filtered_joined_rdd: filtered")

count_rdd = filtered_joined_rdd.map(lambda x: (x[1][0], 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: -x[1])
G.add_node("count_rdd: counted")
G.add_edge("filtered_joined_rdd: filtered", "count_rdd: counted")

for result in count_rdd.take(1):
    print(result[0])

pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, node_size=3000, node_color="skyblue", font_size=10, font_weight="bold", arrows=True)
plt.title('RDD Graph')
plt.show()

sc.stop()

---------------------------

# SQL
# Найти пользователя из РФ, чаще остальных упоминающего фамилии российских политических деятелей (на русском).

import findspark
findspark.init()
findspark.find()

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("csv_processing").getOrCreate()

df = spark.read.csv("ira_tweets_csv_hashed.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("tweets")

politics = ["Путин", "Жириновский", "Зюганов", "Песков", "Лавров", "Пригожин", "Мишустин", "Шойгу", "Медведев", "Матвиенко"]

query = "SELECT userid, COUNT(*) as count FROM tweets WHERE account_language = 'ru' AND tweet_language = 'ru' AND ("
query += f"tweet_text LIKE '%{politics[0]}%'"
for p in politics[1:]:
    query += f" OR tweet_text LIKE '%{p}%'"
query += ') GROUP BY userid ORDER BY count DESC LIMIT 1'

result = spark.sql(query)
result.select('userid').show()

spark.stop()

---------------------------

# RDD
# Найти пользователя из РФ, чаще остальных упоминающего фамилии российских политических деятелей (на русском).

import findspark
findspark.init()
findspark.find()

from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("csv_processing")
sc = SparkContext(conf=conf)

file_path = "ira_tweets_csv_hashed.csv"
rdd = sc.textFile(file_path)

header = rdd.first()
columns = header.split(',')

data_rdd = rdd.filter(lambda line: line != header).map(lambda line: line.split(','))

userid_idx = 1
account_language_idx = 10
tweet_language_idx = 11
tweet_text_idx = 12

politics = ["Путин", "Жириновский", "Зюганов", "Песков", "Лавров", "Пригожин", "Мишустин", "Шойгу", "Медведев", "Матвиенко"]

def contains_politics(text):
    return any(politician in text for politician in politics)

filtered_rdd = data_rdd.filter(lambda x: x[account_language_idx] == '"ru"' and x[tweet_language_idx] == '"ru"' and contains_politics(x[tweet_text_idx])) \
                       .map(lambda x: (x[userid_idx], 1)) \
                       .reduceByKey(lambda a, b: a + b) \
                       .sortBy(lambda x: -x[1])

for result in filtered_rdd.take(1):
    print(result[0])

sc.stop()
